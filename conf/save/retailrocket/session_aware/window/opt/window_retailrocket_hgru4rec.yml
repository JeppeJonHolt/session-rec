type: opt # single|window, maybe add opt
key: hgru4rec #added to the csv names
evaluation: evaluation_user_based #evaluation|evaluation_last|evaluation_multiple|evaluation_user_based|evaluation_user_based_multiple
data:
  name: retailrocket #added in the end of the csv names
  folder: data/retailrocket/prepared_window/
  prefix: events.2
  type: hdf #hdf (if there is no type, the default is csv)
  #opts: {sessions_test: 10}

results:
  folder: results/opt/window/retailrocket/hgru4rec/

metrics:
- class: accuracy_multiple.Precision
  length: [5,10,15,20]
- class: accuracy_multiple.Recall
  length: [5,10,15,20]
- class: accuracy_multiple.MAP
  length: [5,10,15,20]
- class: accuracy.HitRate
  length: [5,10,15,20]
- class: accuracy.MRR
  length: [5,10,15,20]

optimize:
  class: accuracy.MRR
  length: [20]
  iterations: 100 #optional

algorithms:
- class: hgru4rec.hgru4rec.HGRU4Rec
  params: {session_layers: 100, user_layers: 100, loss: 'top1'} # small network, the TOP1 loss always outperformed other ranking losses, so we consider only it
  params_opt:
    final_act: ['linear', 'relu', 'tanh'] # None means default (tanh if the loss is brp or top1; softmax for cross-entropy) # cross-entropy is only affected by 'tanh' where the softmax layers is proceeded by a tanh nonlinearity (default: None)
    dropout_p_hidden_usr: {from: 0.0, to: 0.9, in: 10, type: float}
    dropout_p_hidden_ses: {from: 0.0, to: 0.9, in: 10, type: float}
    dropout_p_init: {from: 0.0, to: 0.9, in: 10, type: float}
    momentum: {from: 0.0, to: 0.9, in: 10, type: float32}
    learning_rate: [ {from: 0.1, to: 0.01, in: 10, type: float32}, {from: 0.5, to: 0.1, in: 5, type: float32} ]
    user_propagation_mode: ['init', 'all']
    batch_size: [50, 100]
  key: hgru4rec